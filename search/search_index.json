{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Calibration Service This backend service is designed to manage calibrations of a hardware device. Each calibration includes: calibration_type value timestamp username Calibrations can be tagged with arbitrary strings to describe different states of a device. Tags can be added or removed from calibrations, and the tagging history is preserved. Read full project overview Pre-requisites Python 3.12 or higher, link uv for python runtime and dependency management, link Node.js for Pyright pre-commit hook execution, link Getting started TODO: (docs): add getting started Test Documentation This repository includes a comprehensive test harness following Clean Architecture principles. The testing infrastructure is designed to be expandable and maintainable. \ud83e\udde0 For detailed information about the testing approach, see docs/TESTS.md . Continuous Integration This project uses GitHub Actions for continuous integration and code quality checks. All workflows use uv for fast dependency resolution. Workflow Status Badge Description CI ( ci.yaml ) Full pipeline: Docker build, integration tests, and coverage reports Python Tests ( pytest.yaml ) Unit/integration tests and coverage reports Type Checking ( pyright.yaml ) Ensures type safety with Pyright Lint & Format ( ruff.yaml ) Enforces consistent style and detects common issues using Ruff \ud83e\udde0 For details, see WORKFLOWS.md .","title":"Calibration Service"},{"location":"#calibration-service","text":"This backend service is designed to manage calibrations of a hardware device. Each calibration includes: calibration_type value timestamp username Calibrations can be tagged with arbitrary strings to describe different states of a device. Tags can be added or removed from calibrations, and the tagging history is preserved. Read full project overview","title":"Calibration Service"},{"location":"#pre-requisites","text":"Python 3.12 or higher, link uv for python runtime and dependency management, link Node.js for Pyright pre-commit hook execution, link","title":"Pre-requisites"},{"location":"#getting-started","text":"TODO: (docs): add getting started","title":"Getting started"},{"location":"#test-documentation","text":"This repository includes a comprehensive test harness following Clean Architecture principles. The testing infrastructure is designed to be expandable and maintainable. \ud83e\udde0 For detailed information about the testing approach, see docs/TESTS.md .","title":"Test Documentation"},{"location":"#continuous-integration","text":"This project uses GitHub Actions for continuous integration and code quality checks. All workflows use uv for fast dependency resolution. Workflow Status Badge Description CI ( ci.yaml ) Full pipeline: Docker build, integration tests, and coverage reports Python Tests ( pytest.yaml ) Unit/integration tests and coverage reports Type Checking ( pyright.yaml ) Ensures type safety with Pyright Lint & Format ( ruff.yaml ) Enforces consistent style and detects common issues using Ruff \ud83e\udde0 For details, see WORKFLOWS.md .","title":"Continuous Integration"},{"location":"ARCHITECTURE/","text":"Architecture Grounding & Inspiration This project uses Clean Architecture patterns. Hereafter \"CA\" will be used for \"Clean Architecture\". Background article by Bob Martin https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html Diagrams The CA Diagram (sphere) - by Uncle Bob - by Uncle Bob The CA Diagram (simplified) - by nikolovlazar Inspiration from existing CA implementations CA with Python - by R. Shaliamekh. clean-architecture-fastapi - by shaliamekh - associated repo for blog post CA in Next.js ](https://www.youtube.com/watch?v=jJVAla0dWJo) - video describing CA nextjs-clean-architecture - by nikolovlazar - associated repo for video Other similar architectures to Clean Architecture Hexagonal Architecture - (a.k.a. Ports and Adapters) by Alistair Cockburn Onion Architecture - by Jeffrey Palermo Screaming Architecture - by Uncle Bob (the same guy behind Clean Architecture) CA in brief Clean Architecture is a set of rules that help us structure our applications in such way that they're easier to maintain and test, and their codebases are predictable. It's like a common language that developers understand, regardless of their technical backgrounds and programming language preferences. Clean Architecture, and similar/derived architectures, all have the same goal - separation of concerns . They introduce layers that bundle similar code together. The \"layering\" helps us achieve important aspects in our codebase: Independent of UI - the business logic is not coupled with the UI framework that's being used (in this case Next.js). The same system can be used in a CLI application, without having to change the business logic or rules. Independent of Database - the database implementation/operations are isolated in their own layer, so the rest of the app does not care about which database is being used, but communicates using Models . Independent of Frameworks - the business rules and logic simply don't know anything about the outside world. They receive data defined with plain objects/dictionaries, Services and Repositories to define their own logic and functionality. This allows us to use frameworks as tools, instead of having to \"mold\" our system into their implementations and limitations. If we use Route Handlers in our app's Drivers , and want to refactor some of them to use a different framework implementation all we need to do is just invoke the specific controllers in the new framework's manner, but the core business logic remains unchanged. Testable - the business logic and rules can easily be tested because it does not depend on the UI framework, or the database, or the web server, or any other external element that builds up our system. Clean Architecture achieves this through defining a dependency hierarchy - layers depend only on layers below them , but not above. Project structure (only the important parts) app - Frameworks & Drivers Layer - basically everything Next.js (pages, server actions, components, styles etc...) or whatever \"consumes\" the app's logic ~~ di - Dependency Injection - a folder where we setup the DI container and the modules~~ # TODO: (future implementation) ~~ db - Everything DB - initializing the DB client, defining schema, migrations~~ # TODO: (future implementation) src - The \"root\" of the system application - Application Layer - holds use cases and interfaces for repositories and services entities - Entities Layer - holds models, value objects and custom errors/exceptions infrastructure - Infrastructure Layer - holds implementations of repositories and services, and pulls in the interfaces from application interface-adapters - Interface Adapters Layer - holds controllers that serve as an entry point to the system (used in Frameworks & Drivers layer to interact with the system) drivers - Frameworks & Drivers layer - holds implementation-specific details for a given framework (e.g. FastAPI routes) tests - Unit tests live here - the unit subfolder's structure matches src pyproject.toml - list of deps & configuration for python tools (linters, formatters, ) ~~Where the pylint-module-boundaries plugin is defined - this stops you from breaking the dependency rule ~~ # TODO: (future implementation) scripts - manual scripts to run project-wide actions (e.g. run tests) from bash terminal docs - project documentation .github/workflows - project cicd files Project structure (Developer Tooling) .pre-commit-config.yaml - rules triggered to run via got hooks cspell.config.yaml - custom dictionary to keep IDE's inspection pane \"clean\" .run - shared run configurations for JetBrains PyCharm IDE Layers explanation Frameworks & Drivers : keeps all the UI framework functionality, and everything else that interacts with the system (eg AWS Lambdas, Stripe webhooks etc...). In this scenario, that's FastAPI Route Handlers This layer should only use Controllers , Models , and Errors , and must not import Use Cases , Repositories , and Services . Interface Adapters : defines Controllers and Presenters : Controllers perform authentication checks and input validation before passing the input to the specific use cases. Controllers orchestrate Use Cases. They don't implement any logic, but define the whole operations using the use cases. In short, they use \"Use Case\" input ports. Errors from deeper layers are bubbled up and being handled where controllers are being used. Controllers use Presenters to convert the data to a UI-friendly format just before returning it to the \"consumer\". This helps prevent leaking any sensitive properties, like emails or hashed passwords, and also helps us slim down the amount of data we're sending back to the client. In short, they implement Use Case output ports. Application : where the business logic lives. Sometimes called core . This layer defines the Use Cases and interfaces for the services and repositories. Use Cases : Represent individual operations, like \"Create Calibration\" or \"Sign In\" Accept pre-validated input (from controllers) and handle authorization checks . Use Repositories and Services to access data sources and communicate with external systems. Use cases should not use other use cases . That's a code smell. It means the use case does multiple things and should be broken down into multiple use cases. Interfaces for Repositories and Services: These are defined in this layer because we want to break out the dependency of their tools and frameworks (database drivers, email services etc...), so we'll implement them in the Infrastructure layer. Since the interfaces live in this layer, use cases (and transitively the upper layers) can access them through Dependency Injection . Dependency Injection allows us to split up the definitions (interfaces) from the implementations (classes) and keep them in a separate layer (infrastructure), but still allow their usage. Entities : where the Models and Exceptions are defined. Models : Define \"domain\" data shapes with plain JavaScript, without using \"database\" technologies. Models are not always tied to the database - sending emails require an external email service, not a database, but we still need to have a data shape that will help other layers communicate \"sending an email\". Models also define their own validation rules, which are called \"Enterprise Business Rules\". Rules that don't usually change, or are least likely to change when something external changes (page navigation, security, etc...). An example is a User model that defines a username field that must be at least 6 characters long and not include special characters . Exceptions : We want our own errors because we don't want to be bubbling up database-specific errors, or any type of errors that are specific to a library or framework. We catch errors that are coming from other libraries (for example SQAlchemy), and convert those errors to our own errors. That's how we can keep our core independent of any frameworks, libraries, and technologies - one of the most important aspects of Clean Architecture. Infrastructure : where Repositories and Services are being defined. This layer pulls in the interfaces of repositories and services from the Application Layer and implements them in their own classes. Repositories are how we implement the database operations. They are classes that expose methods that perform a single database operation - like get_calibration , or create_calibration , or update_calibration . This means that we use the database library / driver in these classes only. They don't perform any data validation, just execute queries and mutations against the database and either throw our custom defined Errors or return results. Services are shared services that are being used across the application - like an authentication service, or email service, or implement external systems like Stripe (create payments, validate receipts etc...). These services also use and depend on other frameworks and libraries. That's why their implementation is kept here alongside the repositories. ~~Since we don't want any layer to depend on this one (and transitively depend on the database and all the services), we use the Dependency Inversion principle . This allows us to depend only on the interfaces defined in the Application Layer , instead of the implementations in the Infrastructure Layer . We use an Inversion of Control library like ioctopus to abstract the implementation behind the interfaces and \"inject\" it whenever we need it. We create the abstraction in the di directory. We \"bind\" the repositories, services, controllers, and use cases to Symbols, and we \"resolve\" them using those symbols when we need the actual implementation. That's how we can use the implementation, without needing to explicitly depend on it (import it).~~ # TODO: future implementation Folder hierarchy with descriptions src/ \u251c\u2500\u2500 config/ # singleton configuration \u2502 \u251c\u2500\u2500 database.py # returns SQLAlchemy SessionLocal instance or Engine \u2502 \u2514\u2500\u2500 environment.py # loads env variables, etc. \u251c\u2500\u2500 entities/ # \"Entities\" or \"core\" layer \u2502 \u251c\u2500\u2500 exceptions.py # Core \"custom\" exceptions, decorates base exception w/\"cause\" and \"messages\" \u2502 \u251c\u2500\u2500 models/ \u2502 \u2514\u2500\u2500 value_objects/ \u251c\u2500\u2500 application/ # \"Application\" layer \u2502 \u251c\u2500\u2500 repositories/ # (interfaces only) \u2502 \u251c\u2500\u2500 services/ # (interfaces only) \u2502 \u2514\u2500\u2500 use_cases/ # implementation of CA \"Use Cases\" \u2502 \u2514\u2500\u2500 exceptions.py # exceptions that are specific to a given \"Use Case\" \u251c\u2500\u2500 interface_adapters/ # \"Interface\" adapters layer from CA \u2502 \u251c\u2500\u2500 controllers/ # \"Controllers\" from CA \u2502 \u2514\u2500\u2500 presenters/ # \"Presenters\" from CA \u251c\u2500\u2500 drivers/ # \"Frameworks & Drivers\" layer \u2502 \u2514\u2500\u2500 rest/ # FastAPI framework \u2502 \u251c\u2500\u2500 routers/ # FastAPI request handlers \u2502 \u251c\u2500\u2500 schemas/ # FastAPI input and output schemas \u2502 \u251c\u2500\u2500 dependencies.py # FastAPI middleware \u2502 \u251c\u2500\u2500 exception_handlers.py # FastAPI exception wrappers \u2502 \u2514\u2500\u2500 main.py # FastAPI entrypoint \u2514\u2500\u2500 infrastructure/ # \"Infrastructure\" layer: implementation of interface from \"Application\" layer \u251c\u2500\u2500 repositories/ \u2502 \u251c\u2500\u2500 calibration_repository/ \u2502 \u2502 \u251c\u2500\u2500 mock_repository.py # implements in-memory \u2502 \u2502 \u251c\u2500\u2500 mongodb_repository.py # implements mongodb \u2502 \u2502 \u2514\u2500\u2500 postgres_repository.py # implements postgres \u2502 \u2514\u2500\u2500 orm_models.py # sqlalchemy-specific orm <-> system entity mapper \u2514\u2500\u2500 services/","title":"Architecture"},{"location":"ARCHITECTURE/#architecture","text":"","title":"Architecture"},{"location":"ARCHITECTURE/#grounding-inspiration","text":"This project uses Clean Architecture patterns. Hereafter \"CA\" will be used for \"Clean Architecture\". Background article by Bob Martin https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html Diagrams The CA Diagram (sphere) - by Uncle Bob - by Uncle Bob The CA Diagram (simplified) - by nikolovlazar Inspiration from existing CA implementations CA with Python - by R. Shaliamekh. clean-architecture-fastapi - by shaliamekh - associated repo for blog post CA in Next.js ](https://www.youtube.com/watch?v=jJVAla0dWJo) - video describing CA nextjs-clean-architecture - by nikolovlazar - associated repo for video Other similar architectures to Clean Architecture Hexagonal Architecture - (a.k.a. Ports and Adapters) by Alistair Cockburn Onion Architecture - by Jeffrey Palermo Screaming Architecture - by Uncle Bob (the same guy behind Clean Architecture)","title":"Grounding &amp; Inspiration"},{"location":"ARCHITECTURE/#ca-in-brief","text":"Clean Architecture is a set of rules that help us structure our applications in such way that they're easier to maintain and test, and their codebases are predictable. It's like a common language that developers understand, regardless of their technical backgrounds and programming language preferences. Clean Architecture, and similar/derived architectures, all have the same goal - separation of concerns . They introduce layers that bundle similar code together. The \"layering\" helps us achieve important aspects in our codebase: Independent of UI - the business logic is not coupled with the UI framework that's being used (in this case Next.js). The same system can be used in a CLI application, without having to change the business logic or rules. Independent of Database - the database implementation/operations are isolated in their own layer, so the rest of the app does not care about which database is being used, but communicates using Models . Independent of Frameworks - the business rules and logic simply don't know anything about the outside world. They receive data defined with plain objects/dictionaries, Services and Repositories to define their own logic and functionality. This allows us to use frameworks as tools, instead of having to \"mold\" our system into their implementations and limitations. If we use Route Handlers in our app's Drivers , and want to refactor some of them to use a different framework implementation all we need to do is just invoke the specific controllers in the new framework's manner, but the core business logic remains unchanged. Testable - the business logic and rules can easily be tested because it does not depend on the UI framework, or the database, or the web server, or any other external element that builds up our system. Clean Architecture achieves this through defining a dependency hierarchy - layers depend only on layers below them , but not above.","title":"CA in brief"},{"location":"ARCHITECTURE/#project-structure-only-the-important-parts","text":"app - Frameworks & Drivers Layer - basically everything Next.js (pages, server actions, components, styles etc...) or whatever \"consumes\" the app's logic ~~ di - Dependency Injection - a folder where we setup the DI container and the modules~~ # TODO: (future implementation) ~~ db - Everything DB - initializing the DB client, defining schema, migrations~~ # TODO: (future implementation) src - The \"root\" of the system application - Application Layer - holds use cases and interfaces for repositories and services entities - Entities Layer - holds models, value objects and custom errors/exceptions infrastructure - Infrastructure Layer - holds implementations of repositories and services, and pulls in the interfaces from application interface-adapters - Interface Adapters Layer - holds controllers that serve as an entry point to the system (used in Frameworks & Drivers layer to interact with the system) drivers - Frameworks & Drivers layer - holds implementation-specific details for a given framework (e.g. FastAPI routes) tests - Unit tests live here - the unit subfolder's structure matches src pyproject.toml - list of deps & configuration for python tools (linters, formatters, ) ~~Where the pylint-module-boundaries plugin is defined - this stops you from breaking the dependency rule ~~ # TODO: (future implementation) scripts - manual scripts to run project-wide actions (e.g. run tests) from bash terminal docs - project documentation .github/workflows - project cicd files","title":"Project structure (only the important parts)"},{"location":"ARCHITECTURE/#project-structure-developer-tooling","text":".pre-commit-config.yaml - rules triggered to run via got hooks cspell.config.yaml - custom dictionary to keep IDE's inspection pane \"clean\" .run - shared run configurations for JetBrains PyCharm IDE","title":"Project structure (Developer Tooling)"},{"location":"ARCHITECTURE/#layers-explanation","text":"Frameworks & Drivers : keeps all the UI framework functionality, and everything else that interacts with the system (eg AWS Lambdas, Stripe webhooks etc...). In this scenario, that's FastAPI Route Handlers This layer should only use Controllers , Models , and Errors , and must not import Use Cases , Repositories , and Services . Interface Adapters : defines Controllers and Presenters : Controllers perform authentication checks and input validation before passing the input to the specific use cases. Controllers orchestrate Use Cases. They don't implement any logic, but define the whole operations using the use cases. In short, they use \"Use Case\" input ports. Errors from deeper layers are bubbled up and being handled where controllers are being used. Controllers use Presenters to convert the data to a UI-friendly format just before returning it to the \"consumer\". This helps prevent leaking any sensitive properties, like emails or hashed passwords, and also helps us slim down the amount of data we're sending back to the client. In short, they implement Use Case output ports. Application : where the business logic lives. Sometimes called core . This layer defines the Use Cases and interfaces for the services and repositories. Use Cases : Represent individual operations, like \"Create Calibration\" or \"Sign In\" Accept pre-validated input (from controllers) and handle authorization checks . Use Repositories and Services to access data sources and communicate with external systems. Use cases should not use other use cases . That's a code smell. It means the use case does multiple things and should be broken down into multiple use cases. Interfaces for Repositories and Services: These are defined in this layer because we want to break out the dependency of their tools and frameworks (database drivers, email services etc...), so we'll implement them in the Infrastructure layer. Since the interfaces live in this layer, use cases (and transitively the upper layers) can access them through Dependency Injection . Dependency Injection allows us to split up the definitions (interfaces) from the implementations (classes) and keep them in a separate layer (infrastructure), but still allow their usage. Entities : where the Models and Exceptions are defined. Models : Define \"domain\" data shapes with plain JavaScript, without using \"database\" technologies. Models are not always tied to the database - sending emails require an external email service, not a database, but we still need to have a data shape that will help other layers communicate \"sending an email\". Models also define their own validation rules, which are called \"Enterprise Business Rules\". Rules that don't usually change, or are least likely to change when something external changes (page navigation, security, etc...). An example is a User model that defines a username field that must be at least 6 characters long and not include special characters . Exceptions : We want our own errors because we don't want to be bubbling up database-specific errors, or any type of errors that are specific to a library or framework. We catch errors that are coming from other libraries (for example SQAlchemy), and convert those errors to our own errors. That's how we can keep our core independent of any frameworks, libraries, and technologies - one of the most important aspects of Clean Architecture. Infrastructure : where Repositories and Services are being defined. This layer pulls in the interfaces of repositories and services from the Application Layer and implements them in their own classes. Repositories are how we implement the database operations. They are classes that expose methods that perform a single database operation - like get_calibration , or create_calibration , or update_calibration . This means that we use the database library / driver in these classes only. They don't perform any data validation, just execute queries and mutations against the database and either throw our custom defined Errors or return results. Services are shared services that are being used across the application - like an authentication service, or email service, or implement external systems like Stripe (create payments, validate receipts etc...). These services also use and depend on other frameworks and libraries. That's why their implementation is kept here alongside the repositories. ~~Since we don't want any layer to depend on this one (and transitively depend on the database and all the services), we use the Dependency Inversion principle . This allows us to depend only on the interfaces defined in the Application Layer , instead of the implementations in the Infrastructure Layer . We use an Inversion of Control library like ioctopus to abstract the implementation behind the interfaces and \"inject\" it whenever we need it. We create the abstraction in the di directory. We \"bind\" the repositories, services, controllers, and use cases to Symbols, and we \"resolve\" them using those symbols when we need the actual implementation. That's how we can use the implementation, without needing to explicitly depend on it (import it).~~ # TODO: future implementation","title":"Layers explanation"},{"location":"ARCHITECTURE/#folder-hierarchy-with-descriptions","text":"src/ \u251c\u2500\u2500 config/ # singleton configuration \u2502 \u251c\u2500\u2500 database.py # returns SQLAlchemy SessionLocal instance or Engine \u2502 \u2514\u2500\u2500 environment.py # loads env variables, etc. \u251c\u2500\u2500 entities/ # \"Entities\" or \"core\" layer \u2502 \u251c\u2500\u2500 exceptions.py # Core \"custom\" exceptions, decorates base exception w/\"cause\" and \"messages\" \u2502 \u251c\u2500\u2500 models/ \u2502 \u2514\u2500\u2500 value_objects/ \u251c\u2500\u2500 application/ # \"Application\" layer \u2502 \u251c\u2500\u2500 repositories/ # (interfaces only) \u2502 \u251c\u2500\u2500 services/ # (interfaces only) \u2502 \u2514\u2500\u2500 use_cases/ # implementation of CA \"Use Cases\" \u2502 \u2514\u2500\u2500 exceptions.py # exceptions that are specific to a given \"Use Case\" \u251c\u2500\u2500 interface_adapters/ # \"Interface\" adapters layer from CA \u2502 \u251c\u2500\u2500 controllers/ # \"Controllers\" from CA \u2502 \u2514\u2500\u2500 presenters/ # \"Presenters\" from CA \u251c\u2500\u2500 drivers/ # \"Frameworks & Drivers\" layer \u2502 \u2514\u2500\u2500 rest/ # FastAPI framework \u2502 \u251c\u2500\u2500 routers/ # FastAPI request handlers \u2502 \u251c\u2500\u2500 schemas/ # FastAPI input and output schemas \u2502 \u251c\u2500\u2500 dependencies.py # FastAPI middleware \u2502 \u251c\u2500\u2500 exception_handlers.py # FastAPI exception wrappers \u2502 \u2514\u2500\u2500 main.py # FastAPI entrypoint \u2514\u2500\u2500 infrastructure/ # \"Infrastructure\" layer: implementation of interface from \"Application\" layer \u251c\u2500\u2500 repositories/ \u2502 \u251c\u2500\u2500 calibration_repository/ \u2502 \u2502 \u251c\u2500\u2500 mock_repository.py # implements in-memory \u2502 \u2502 \u251c\u2500\u2500 mongodb_repository.py # implements mongodb \u2502 \u2502 \u2514\u2500\u2500 postgres_repository.py # implements postgres \u2502 \u2514\u2500\u2500 orm_models.py # sqlalchemy-specific orm <-> system entity mapper \u2514\u2500\u2500 services/","title":"Folder hierarchy with descriptions"},{"location":"DATABASE/","text":"Database Management This document covers database management, migrations, and best practices for this project. Overview The project uses: PostgreSQL as the database SQLAlchemy for ORM and database operations Alembic for database migrations Automatic migration generation from SQLAlchemy models Directory Structure src/ \u251c\u2500\u2500 infrastructure/ \u2502 \u2514\u2500\u2500 orm_models.py # SQLAlchemy models \u251c\u2500\u2500 config/ \u2502 \u2514\u2500\u2500 database.py # Database configuration \u2514\u2500\u2500 alembic/ \u251c\u2500\u2500 versions/ # Migration version files \u251c\u2500\u2500 env.py # Migration environment configuration \u2514\u2500\u2500 script.py.mako # Migration script template Common Operations The project provides several commands for database management: # Initialize database and run migrations uv run db_init # Create a new migration after model changes uv run db_create \"Description of changes\" # Apply pending migrations uv run db_migrate Migration Management Creating Migrations When you modify SQLAlchemy models in orm_models.py , create a new migration: uv run db_create \"Add status column to Calibration model\" Always: Review the generated migration in alembic/versions/ Check indexes and constraints Add any necessary data migrations Run tests before committing Applying Migrations To apply pending migrations: # Development environment uv run db_migrate # Test environment PYTHON_ENV=test uv run db_migrate # Production environment PYTHON_ENV=production uv run db_migrate Advanced Operations For advanced cases, you can use direct Alembic commands: # Roll back one migration PYTHONPATH=./src alembic downgrade -1 # View migration history PYTHONPATH=./src alembic history --verbose # Roll back to specific version PYTHONPATH=./src alembic downgrade <version_id> Best Practices 1. Migration Management One migration per model change Write descriptive migration messages Include model name in migration message Review generated migrations before committing Test both upgrade and downgrade paths 2. Data Safety Always backup production data before migrations Test migrations on development first Include data migrations when needed Verify data integrity after migration 3. Development Workflow Create migrations in feature branches Run full test suite after migrations Document significant migrations Use appropriate environment for testing 4. Error Handling Never edit committed migrations Create new migrations for fixes Roll back failed migrations cleanly Check database state before retrying Environment Configuration The database system uses the project's layered environment configuration: .env - Base defaults (in git) .env.local - Local overrides (git-ignored) .env.{ENV} - Environment specific (in git) .env.{ENV}.local - Environment specific local overrides (git-ignored) Example configurations: # .env (base defaults) DATABASE_URL=postgresql+asyncpg://dev-user:password@localhost:5432/dev_db # .env.test DATABASE_URL=postgresql+asyncpg://test-user:password@localhost:5432/test_db # .env.production DATABASE_URL=postgresql+asyncpg://app-user:password@db.example.com:5432/prod_db Troubleshooting 1. Migration Not Detected Ensure model is imported in orm_models.py Verify model inherits from Base Check for circular imports Run with --verbose flag for more details 2. Connection Issues # Verify database is running docker compose ps # Check connection uv run db_init # Reset database (development only) docker compose down -v postgres uv run db_init 3. Migration Conflicts Never edit committed migrations Create new migrations for fixes Use alembic history to check state Consider rolling back if needed 4. Environment Issues # Check current settings cat .env cat .env.local # If it exists # Verify environment echo $PYTHON_ENV echo $DATABASE_URL For more information about development workflows, see DEVELOPER.md .","title":"Database Management"},{"location":"DATABASE/#database-management","text":"This document covers database management, migrations, and best practices for this project.","title":"Database Management"},{"location":"DATABASE/#overview","text":"The project uses: PostgreSQL as the database SQLAlchemy for ORM and database operations Alembic for database migrations Automatic migration generation from SQLAlchemy models","title":"Overview"},{"location":"DATABASE/#directory-structure","text":"src/ \u251c\u2500\u2500 infrastructure/ \u2502 \u2514\u2500\u2500 orm_models.py # SQLAlchemy models \u251c\u2500\u2500 config/ \u2502 \u2514\u2500\u2500 database.py # Database configuration \u2514\u2500\u2500 alembic/ \u251c\u2500\u2500 versions/ # Migration version files \u251c\u2500\u2500 env.py # Migration environment configuration \u2514\u2500\u2500 script.py.mako # Migration script template","title":"Directory Structure"},{"location":"DATABASE/#common-operations","text":"The project provides several commands for database management: # Initialize database and run migrations uv run db_init # Create a new migration after model changes uv run db_create \"Description of changes\" # Apply pending migrations uv run db_migrate","title":"Common Operations"},{"location":"DATABASE/#migration-management","text":"","title":"Migration Management"},{"location":"DATABASE/#creating-migrations","text":"When you modify SQLAlchemy models in orm_models.py , create a new migration: uv run db_create \"Add status column to Calibration model\" Always: Review the generated migration in alembic/versions/ Check indexes and constraints Add any necessary data migrations Run tests before committing","title":"Creating Migrations"},{"location":"DATABASE/#applying-migrations","text":"To apply pending migrations: # Development environment uv run db_migrate # Test environment PYTHON_ENV=test uv run db_migrate # Production environment PYTHON_ENV=production uv run db_migrate","title":"Applying Migrations"},{"location":"DATABASE/#advanced-operations","text":"For advanced cases, you can use direct Alembic commands: # Roll back one migration PYTHONPATH=./src alembic downgrade -1 # View migration history PYTHONPATH=./src alembic history --verbose # Roll back to specific version PYTHONPATH=./src alembic downgrade <version_id>","title":"Advanced Operations"},{"location":"DATABASE/#best-practices","text":"","title":"Best Practices"},{"location":"DATABASE/#1-migration-management","text":"One migration per model change Write descriptive migration messages Include model name in migration message Review generated migrations before committing Test both upgrade and downgrade paths","title":"1. Migration Management"},{"location":"DATABASE/#2-data-safety","text":"Always backup production data before migrations Test migrations on development first Include data migrations when needed Verify data integrity after migration","title":"2. Data Safety"},{"location":"DATABASE/#3-development-workflow","text":"Create migrations in feature branches Run full test suite after migrations Document significant migrations Use appropriate environment for testing","title":"3. Development Workflow"},{"location":"DATABASE/#4-error-handling","text":"Never edit committed migrations Create new migrations for fixes Roll back failed migrations cleanly Check database state before retrying","title":"4. Error Handling"},{"location":"DATABASE/#environment-configuration","text":"The database system uses the project's layered environment configuration: .env - Base defaults (in git) .env.local - Local overrides (git-ignored) .env.{ENV} - Environment specific (in git) .env.{ENV}.local - Environment specific local overrides (git-ignored) Example configurations: # .env (base defaults) DATABASE_URL=postgresql+asyncpg://dev-user:password@localhost:5432/dev_db # .env.test DATABASE_URL=postgresql+asyncpg://test-user:password@localhost:5432/test_db # .env.production DATABASE_URL=postgresql+asyncpg://app-user:password@db.example.com:5432/prod_db","title":"Environment Configuration"},{"location":"DATABASE/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"DATABASE/#1-migration-not-detected","text":"Ensure model is imported in orm_models.py Verify model inherits from Base Check for circular imports Run with --verbose flag for more details","title":"1. Migration Not Detected"},{"location":"DATABASE/#2-connection-issues","text":"# Verify database is running docker compose ps # Check connection uv run db_init # Reset database (development only) docker compose down -v postgres uv run db_init","title":"2. Connection Issues"},{"location":"DATABASE/#3-migration-conflicts","text":"Never edit committed migrations Create new migrations for fixes Use alembic history to check state Consider rolling back if needed","title":"3. Migration Conflicts"},{"location":"DATABASE/#4-environment-issues","text":"# Check current settings cat .env cat .env.local # If it exists # Verify environment echo $PYTHON_ENV echo $DATABASE_URL For more information about development workflows, see DEVELOPER.md .","title":"4. Environment Issues"},{"location":"DEVELOPER/","text":"Developer Guide This guide covers day-to-day development workflows and commands. For initial setup instructions, see the README.md . Environment Configuration The project uses layered environment files for configuration: .env - Base defaults (in git) .env.local - Local overrides (git-ignored) .env.{ENV} - Environment specific (in git) .env.{ENV}.local - Environment specific local overrides (git-ignored) Environment files are loaded in the order above, with later files overriding earlier ones. The {ENV} placeholder is replaced with the value of PYTHON_ENV (defaults to \"development\"). Example: To run in test environment: PYTHON_ENV=test uv run db_init # Will use .env \u2192 .env.local \u2192 .env.test \u2192 .env.test.local Pre-commit Hooks The project uses pre-commit hooks to ensure code quality. These are configured in .pre-commit-config.yaml . Installed Hooks Hook Description When it Runs conventional-pre-commit Enforces Conventional Commits format On commit message run_tests Runs project tests Before commit trailing-whitespace Removes trailing whitespace Before commit end-of-file-fixer Ensures files end with newline Before commit check-toml Validates TOML files Before commit check-yaml Validates YAML files Before commit check-added-large-files Prevents large file commits Before commit ruff Lints Python code Before commit ruff-format Formats Python code Before commit pyright Type checks Python code Before commit uv-lock Updates dependency lock file Before commit Using Pre-commit # Install the hooks pre-commit install --install-hooks # Run all hooks manually. Note: errors in alembic directory should be ignored. pre-commit run --all-files # Run specific hook pre-commit run ruff --all-files Commit Message Format We use conventional commits with the following types: feat : New feature fix : Bug fix refactor : Code change that neither fixes a bug nor adds a feature test : Adding missing tests or correcting existing tests style : Changes that do not affect the meaning of the code chore : Other changes that don't modify src or test files build : Changes that affect the build system or external dependencies ci : Changes to CI configuration files and scripts docs : Documentation only changes wip : Work in progress Example: git commit -m \"feat: add user authentication\" Available Commands All commands are run using uv run <command> . Here's a complete list: Command Description Common Use dev Start the development server Local development check Runs all \"before committing\" cmds Local development format Run code formatters Before committing lint Run linters Before committing test Run unit and integration tests Before committing test_debug Run test and prints log_test_step output Debugging failed tests test_cov Run tests with coverage report CI/CD, coverage checks typecheck Run type checker Before committing db_init Initialize database and run migrations First-time setup, reset DB db_migrate Apply pending migrations After pulling new migrations db_create Create a new migration After model changes setup Full setup (init DB and run tests) First-time setup, CI/CD cp_iso Copy ISO timestamp to clipboard Creating timestamps cp_ulid Copy ULID to clipboard Creating unique IDs cp_uuid Copy UUID to clipboard Creating unique IDs Common Development Workflows 1. Daily Development Start your day with: # Pull latest changes git pull # Apply any new migrations uv run db_migrate # Start the development server uv run dev 2. Making Changes When modifying code: # Format and lint code, check types, run tests uv run check 3. Database Changes When modifying models: # Create a new migration uv run db_create \"Description of your changes\" # Apply the migration uv run db_migrate # Run tests to ensure everything works uv run test 4. Pre-commit Checklist The pre-commit hooks will run automatically, but you can also run them manually: # Run all pre-commit hooks uv run pre-commit run --all-files # If all pass, commit your changes git add . git commit -m \"feat: your feature description\" 5. Running Tests Different test scenarios: # Run all tests uv run test # Run with coverage uv run test_cov # Run in test environment PYTHON_ENV=test uv run test Best Practices Environment Variables Never commit sensitive values to git Use .env.local for local development overrides Use environment-specific files for different configurations Database Migrations One migration per model change Write meaningful migration messages Always run tests after applying migrations Code Quality Let pre-commit hooks run automatically Fix issues before committing Keep test coverage high Testing Write tests before fixing bugs Update tests when changing features Use appropriate test environment Troubleshooting Common issues and solutions: Database Connection Issues bash # Reset database and migrations uv run db_init Environment Issues bash # Verify environment cat .env cat .env.local # If it exists Pre-commit Hook Issues bash # Update hooks to latest version pre-commit clean pre-commit install --install-hooks Test Database Issues bash # Run with test environment PYTHON_ENV=test uv run db_init PYTHON_ENV=test uv run test For more detailed information about the architecture and design decisions, see ARCHITECTURE.md .","title":"Developers"},{"location":"DEVELOPER/#developer-guide","text":"This guide covers day-to-day development workflows and commands. For initial setup instructions, see the README.md .","title":"Developer Guide"},{"location":"DEVELOPER/#environment-configuration","text":"The project uses layered environment files for configuration: .env - Base defaults (in git) .env.local - Local overrides (git-ignored) .env.{ENV} - Environment specific (in git) .env.{ENV}.local - Environment specific local overrides (git-ignored) Environment files are loaded in the order above, with later files overriding earlier ones. The {ENV} placeholder is replaced with the value of PYTHON_ENV (defaults to \"development\"). Example: To run in test environment: PYTHON_ENV=test uv run db_init # Will use .env \u2192 .env.local \u2192 .env.test \u2192 .env.test.local","title":"Environment Configuration"},{"location":"DEVELOPER/#pre-commit-hooks","text":"The project uses pre-commit hooks to ensure code quality. These are configured in .pre-commit-config.yaml .","title":"Pre-commit Hooks"},{"location":"DEVELOPER/#installed-hooks","text":"Hook Description When it Runs conventional-pre-commit Enforces Conventional Commits format On commit message run_tests Runs project tests Before commit trailing-whitespace Removes trailing whitespace Before commit end-of-file-fixer Ensures files end with newline Before commit check-toml Validates TOML files Before commit check-yaml Validates YAML files Before commit check-added-large-files Prevents large file commits Before commit ruff Lints Python code Before commit ruff-format Formats Python code Before commit pyright Type checks Python code Before commit uv-lock Updates dependency lock file Before commit","title":"Installed Hooks"},{"location":"DEVELOPER/#using-pre-commit","text":"# Install the hooks pre-commit install --install-hooks # Run all hooks manually. Note: errors in alembic directory should be ignored. pre-commit run --all-files # Run specific hook pre-commit run ruff --all-files","title":"Using Pre-commit"},{"location":"DEVELOPER/#commit-message-format","text":"We use conventional commits with the following types: feat : New feature fix : Bug fix refactor : Code change that neither fixes a bug nor adds a feature test : Adding missing tests or correcting existing tests style : Changes that do not affect the meaning of the code chore : Other changes that don't modify src or test files build : Changes that affect the build system or external dependencies ci : Changes to CI configuration files and scripts docs : Documentation only changes wip : Work in progress Example: git commit -m \"feat: add user authentication\"","title":"Commit Message Format"},{"location":"DEVELOPER/#available-commands","text":"All commands are run using uv run <command> . Here's a complete list: Command Description Common Use dev Start the development server Local development check Runs all \"before committing\" cmds Local development format Run code formatters Before committing lint Run linters Before committing test Run unit and integration tests Before committing test_debug Run test and prints log_test_step output Debugging failed tests test_cov Run tests with coverage report CI/CD, coverage checks typecheck Run type checker Before committing db_init Initialize database and run migrations First-time setup, reset DB db_migrate Apply pending migrations After pulling new migrations db_create Create a new migration After model changes setup Full setup (init DB and run tests) First-time setup, CI/CD cp_iso Copy ISO timestamp to clipboard Creating timestamps cp_ulid Copy ULID to clipboard Creating unique IDs cp_uuid Copy UUID to clipboard Creating unique IDs","title":"Available Commands"},{"location":"DEVELOPER/#common-development-workflows","text":"","title":"Common Development Workflows"},{"location":"DEVELOPER/#1-daily-development","text":"Start your day with: # Pull latest changes git pull # Apply any new migrations uv run db_migrate # Start the development server uv run dev","title":"1. Daily Development"},{"location":"DEVELOPER/#2-making-changes","text":"When modifying code: # Format and lint code, check types, run tests uv run check","title":"2. Making Changes"},{"location":"DEVELOPER/#3-database-changes","text":"When modifying models: # Create a new migration uv run db_create \"Description of your changes\" # Apply the migration uv run db_migrate # Run tests to ensure everything works uv run test","title":"3. Database Changes"},{"location":"DEVELOPER/#4-pre-commit-checklist","text":"The pre-commit hooks will run automatically, but you can also run them manually: # Run all pre-commit hooks uv run pre-commit run --all-files # If all pass, commit your changes git add . git commit -m \"feat: your feature description\"","title":"4. Pre-commit Checklist"},{"location":"DEVELOPER/#5-running-tests","text":"Different test scenarios: # Run all tests uv run test # Run with coverage uv run test_cov # Run in test environment PYTHON_ENV=test uv run test","title":"5. Running Tests"},{"location":"DEVELOPER/#best-practices","text":"Environment Variables Never commit sensitive values to git Use .env.local for local development overrides Use environment-specific files for different configurations Database Migrations One migration per model change Write meaningful migration messages Always run tests after applying migrations Code Quality Let pre-commit hooks run automatically Fix issues before committing Keep test coverage high Testing Write tests before fixing bugs Update tests when changing features Use appropriate test environment","title":"Best Practices"},{"location":"DEVELOPER/#troubleshooting","text":"Common issues and solutions: Database Connection Issues bash # Reset database and migrations uv run db_init Environment Issues bash # Verify environment cat .env cat .env.local # If it exists Pre-commit Hook Issues bash # Update hooks to latest version pre-commit clean pre-commit install --install-hooks Test Database Issues bash # Run with test environment PYTHON_ENV=test uv run db_init PYTHON_ENV=test uv run test For more detailed information about the architecture and design decisions, see ARCHITECTURE.md .","title":"Troubleshooting"},{"location":"PROJECT/","text":"Senior Backend Engineer - ISW | Take Home Overview This backend service is designed to manage calibrations of a hardware device. Each calibration includes: calibration_type value timestamp username Calibrations can be tagged with arbitrary strings to describe different states of a device. Tags can be added or removed from calibrations, and the tagging history is preserved. Technology Stack Language: Python Database: PostgreSQL or SQLite Libraries: See: pyproject.toml Use Cases & API Endpoints 1. Create a New Calibration Use Case: AddCalibrationUseCase application.use_cases.calibrations.add_calibration.py Endpoint: POST /calibrations Input: { \"calibration_type\": \"string\", \"value\": \"float\", \"timestamp\": \"string (ISO 8601)\", \"username\": \"string\" } Output: { \"calibration_id\": \"integer\" } 2. Query Calibrations by Filter list_calibrations_use_case Use Case: ListCalibrationsUseCase application.use_cases.calibrations.list_calibrations.py Endpoint: GET /calibrations Query Parameters: username : string timestamp : string (ISO 8601) calibration_type : string Output: [ { \"calibration_id\": \"integer\", \"calibration_type\": \"string\", \"value\": \"float\", \"timestamp\": \"string (ISO 8601)\", \"username\": \"string\" } ] 3. Tagging Support 3a. Add a tag to a Calibration Use Case: AddTagToCalibrationUseCase application.use_cases.tags.add_tag_to_calibration.py Endpoint: POST /calibrations/{calibration_id}/tags Input: { \"tag\": \"string\" } Output: body json { \"message\": \"Tag added successfully\" } 3b. Removing a tag Use Case: RemoveTagFromCalibrationUseCase application.use_cases.tags.remove_tag_from_calibration.py Endpoint: DELETE /calibrations/{calibration_id}/tags/{tag_name} application.use_cases.tags.remove_tag_from_calibration Input: path: tag Output: body: json { \"message\": \"Tag removed successfully\" } Notes: Each calibration can be tagged and untagged with any number of tags Each tag is an arbitrary string i.e. the tags are not pre-defined (examples: \"current-state\" , \"baseline-2025\" ) Whenever a calibration is tagged or untagged those times are recorded 4. Retrieve Calibrations by Tag Use Case : GetCalibrationsByTagUseCase - application.use_cases.tags.get_calibrations_by_tag.py Endpoint: GET /tags/{tag}/calibrations Query Parameters: timestamp : string (ISO 8601) username : string Output: body TODO: (docs): add Response Schema Notes: Output: The list of calibrations associated with that tag at that time (i.e., calibrations added to the tag at or before that time, and not removed before that time) 5. Query Tags Associated with a Calibration Use Case : GetTagsForCalibrationUseCase - application.use_cases.calibrations.get_tags_for_calibration.py Endpoint: GET /calibrations/{calibration_id}/tags Query Parameters: timestamp : string (ISO 8601) Output: [ \"tag1\", \"tag2\", \"tag3\" ] Notes: Given a calibration primary key and a timestamp, retrieve all the tags that it was a part of at that time Sample Calibration Data [ { \"calibration_type\": \"offset\", \"value\": 1.0, \"username\": \"alice\" }, { \"calibration_type\": \"gain\", \"value\": 1.5, \"username\": \"bob\" }, { \"calibration_type\": \"temperature\", \"value\": -0.3, \"username\": \"charlie\" }, { \"calibration_type\": \"offset\", \"value\": 0.9, \"username\": \"alice\" }, { \"calibration_type\": \"gain\", \"value\": 1.6, \"username\": \"dana\" }, { \"calibration_type\": \"offset\", \"value\": 1.2, \"username\": \"bob\" }, { \"calibration_type\": \"gain\", \"value\": 1.55, \"username\": \"alice\" }, { \"calibration_type\": \"temperature\", \"value\": -0.1, \"username\": \"charlie\" }, { \"calibration_type\": \"pressure\", \"value\": 101.3, \"username\": \"dana\" }, { \"calibration_type\": \"offset\", \"value\": 1.1, \"username\": \"alice\" }, { \"calibration_type\": \"gain\", \"value\": 1.4, \"username\": \"bob\" }, { \"calibration_type\": \"offset\", \"value\": 1.3, \"username\": \"charlie\" }, { \"calibration_type\": \"temperature\", \"value\": -0.2, \"username\": \"alice\" }, { \"calibration_type\": \"pressure\", \"value\": 100.8, \"username\": \"bob\" }, { \"calibration_type\": \"gain\", \"value\": 1.6, \"username\": \"charlie\" } ]","title":"Project"},{"location":"PROJECT/#senior-backend-engineer-isw-take-home","text":"","title":"Senior Backend Engineer - ISW | Take Home"},{"location":"PROJECT/#overview","text":"This backend service is designed to manage calibrations of a hardware device. Each calibration includes: calibration_type value timestamp username Calibrations can be tagged with arbitrary strings to describe different states of a device. Tags can be added or removed from calibrations, and the tagging history is preserved.","title":"Overview"},{"location":"PROJECT/#technology-stack","text":"Language: Python Database: PostgreSQL or SQLite Libraries: See: pyproject.toml","title":"Technology Stack"},{"location":"PROJECT/#use-cases-api-endpoints","text":"","title":"Use Cases &amp; API Endpoints"},{"location":"PROJECT/#1-create-a-new-calibration","text":"Use Case: AddCalibrationUseCase application.use_cases.calibrations.add_calibration.py Endpoint: POST /calibrations Input: { \"calibration_type\": \"string\", \"value\": \"float\", \"timestamp\": \"string (ISO 8601)\", \"username\": \"string\" } Output: { \"calibration_id\": \"integer\" }","title":"1. Create a New Calibration"},{"location":"PROJECT/#2-query-calibrations-by-filter-list_calibrations_use_case","text":"Use Case: ListCalibrationsUseCase application.use_cases.calibrations.list_calibrations.py Endpoint: GET /calibrations Query Parameters: username : string timestamp : string (ISO 8601) calibration_type : string Output: [ { \"calibration_id\": \"integer\", \"calibration_type\": \"string\", \"value\": \"float\", \"timestamp\": \"string (ISO 8601)\", \"username\": \"string\" } ]","title":"2. Query Calibrations by Filter list_calibrations_use_case"},{"location":"PROJECT/#3-tagging-support","text":"","title":"3. Tagging Support"},{"location":"PROJECT/#3a-add-a-tag-to-a-calibration","text":"Use Case: AddTagToCalibrationUseCase application.use_cases.tags.add_tag_to_calibration.py Endpoint: POST /calibrations/{calibration_id}/tags Input: { \"tag\": \"string\" } Output: body json { \"message\": \"Tag added successfully\" }","title":"3a. Add a tag to a Calibration"},{"location":"PROJECT/#3b-removing-a-tag","text":"Use Case: RemoveTagFromCalibrationUseCase application.use_cases.tags.remove_tag_from_calibration.py Endpoint: DELETE /calibrations/{calibration_id}/tags/{tag_name} application.use_cases.tags.remove_tag_from_calibration Input: path: tag Output: body: json { \"message\": \"Tag removed successfully\" } Notes: Each calibration can be tagged and untagged with any number of tags Each tag is an arbitrary string i.e. the tags are not pre-defined (examples: \"current-state\" , \"baseline-2025\" ) Whenever a calibration is tagged or untagged those times are recorded","title":"3b. Removing a tag"},{"location":"PROJECT/#4-retrieve-calibrations-by-tag","text":"Use Case : GetCalibrationsByTagUseCase - application.use_cases.tags.get_calibrations_by_tag.py Endpoint: GET /tags/{tag}/calibrations Query Parameters: timestamp : string (ISO 8601) username : string Output: body","title":"4. Retrieve Calibrations by Tag"},{"location":"PROJECT/#todo-docs-add-response-schema","text":"Notes: Output: The list of calibrations associated with that tag at that time (i.e., calibrations added to the tag at or before that time, and not removed before that time)","title":"TODO: (docs): add Response Schema"},{"location":"PROJECT/#5-query-tags-associated-with-a-calibration","text":"Use Case : GetTagsForCalibrationUseCase - application.use_cases.calibrations.get_tags_for_calibration.py Endpoint: GET /calibrations/{calibration_id}/tags Query Parameters: timestamp : string (ISO 8601) Output: [ \"tag1\", \"tag2\", \"tag3\" ] Notes: Given a calibration primary key and a timestamp, retrieve all the tags that it was a part of at that time","title":"5. Query Tags Associated with a Calibration"},{"location":"PROJECT/#sample-calibration-data","text":"[ { \"calibration_type\": \"offset\", \"value\": 1.0, \"username\": \"alice\" }, { \"calibration_type\": \"gain\", \"value\": 1.5, \"username\": \"bob\" }, { \"calibration_type\": \"temperature\", \"value\": -0.3, \"username\": \"charlie\" }, { \"calibration_type\": \"offset\", \"value\": 0.9, \"username\": \"alice\" }, { \"calibration_type\": \"gain\", \"value\": 1.6, \"username\": \"dana\" }, { \"calibration_type\": \"offset\", \"value\": 1.2, \"username\": \"bob\" }, { \"calibration_type\": \"gain\", \"value\": 1.55, \"username\": \"alice\" }, { \"calibration_type\": \"temperature\", \"value\": -0.1, \"username\": \"charlie\" }, { \"calibration_type\": \"pressure\", \"value\": 101.3, \"username\": \"dana\" }, { \"calibration_type\": \"offset\", \"value\": 1.1, \"username\": \"alice\" }, { \"calibration_type\": \"gain\", \"value\": 1.4, \"username\": \"bob\" }, { \"calibration_type\": \"offset\", \"value\": 1.3, \"username\": \"charlie\" }, { \"calibration_type\": \"temperature\", \"value\": -0.2, \"username\": \"alice\" }, { \"calibration_type\": \"pressure\", \"value\": 100.8, \"username\": \"bob\" }, { \"calibration_type\": \"gain\", \"value\": 1.6, \"username\": \"charlie\" } ]","title":"Sample Calibration Data"},{"location":"TESTS/","text":"Testing Strategy for Calibration Service This document outlines the testing strategy for the Calibration Service, following Clean Architecture principles. Test Structure The test suite is organized into the following categories: Unit Tests : Testing individual components in isolation tests/unit/entities : Tests for entity models and validation tests/unit/application : Tests for use cases and business logic tests/unit/infrastructure : Tests for repositories and services tests/unit/interface_adapters : Tests for controllers tests/unit/use_cases : Tests for use cases Integration Tests : Testing the interaction between components tests/integration : API endpoint testing with in-memory repositories End-to-End Tests : Testing the entire system (future implementation) tests/e2e : Full system tests with external dependencies Test Dependencies The test suite uses the following dependencies: pytest : The core testing framework httpx : HTTP client for testing FastAPI endpoints pytest-asyncio : Support for asynchronous tests pytest-cov : Code coverage reporting Running Tests Tests can be run using the run_tests.sh script: bash ./scripts/run_tests.sh [!NOTE] run_tests is executed by pre-commit as a pre-commit hook. Generate coverage reports using the run_tests_with_coverage.sh script: bash ./scripts/run_tests_with_coverage.sh Or, run manually: ```bash # Run all tests python -m pytest -v --no-cov # Run unit tests only python -m pytest tests/unit -v --no-cov # Run integration tests only python -m pytest tests/integration -v --no-cov # Run with coverage (pytest.ini passes coverage flags by default) python -m pytest ``` Test Data Unit Tests : Use mock objects and in-memory repositories Integration Tests : Use in-memory repositories for isolation E2E Tests : Will use test databases or containerized services Clean Architecture Testing Approach Our testing follows Clean Architecture principles: Entities Tests : Verify that our core models work correctly Use Case Tests : Ensure business rules are correctly implemented Interface Adapter Tests : Validate controllers and presenters Framework Tests : Test the FastAPI endpoints By following this approach, we can ensure that our business logic remains independent of frameworks and external concerns, making our tests more robust and our code more maintainable.","title":"Testing"},{"location":"TESTS/#testing-strategy-for-calibration-service","text":"This document outlines the testing strategy for the Calibration Service, following Clean Architecture principles.","title":"Testing Strategy for Calibration Service"},{"location":"TESTS/#test-structure","text":"The test suite is organized into the following categories: Unit Tests : Testing individual components in isolation tests/unit/entities : Tests for entity models and validation tests/unit/application : Tests for use cases and business logic tests/unit/infrastructure : Tests for repositories and services tests/unit/interface_adapters : Tests for controllers tests/unit/use_cases : Tests for use cases Integration Tests : Testing the interaction between components tests/integration : API endpoint testing with in-memory repositories End-to-End Tests : Testing the entire system (future implementation) tests/e2e : Full system tests with external dependencies","title":"Test Structure"},{"location":"TESTS/#test-dependencies","text":"The test suite uses the following dependencies: pytest : The core testing framework httpx : HTTP client for testing FastAPI endpoints pytest-asyncio : Support for asynchronous tests pytest-cov : Code coverage reporting","title":"Test Dependencies"},{"location":"TESTS/#running-tests","text":"Tests can be run using the run_tests.sh script: bash ./scripts/run_tests.sh [!NOTE] run_tests is executed by pre-commit as a pre-commit hook. Generate coverage reports using the run_tests_with_coverage.sh script: bash ./scripts/run_tests_with_coverage.sh Or, run manually: ```bash # Run all tests python -m pytest -v --no-cov # Run unit tests only python -m pytest tests/unit -v --no-cov # Run integration tests only python -m pytest tests/integration -v --no-cov # Run with coverage (pytest.ini passes coverage flags by default) python -m pytest ```","title":"Running Tests"},{"location":"TESTS/#test-data","text":"Unit Tests : Use mock objects and in-memory repositories Integration Tests : Use in-memory repositories for isolation E2E Tests : Will use test databases or containerized services","title":"Test Data"},{"location":"TESTS/#clean-architecture-testing-approach","text":"Our testing follows Clean Architecture principles: Entities Tests : Verify that our core models work correctly Use Case Tests : Ensure business rules are correctly implemented Interface Adapter Tests : Validate controllers and presenters Framework Tests : Test the FastAPI endpoints By following this approach, we can ensure that our business logic remains independent of frameworks and external concerns, making our tests more robust and our code more maintainable.","title":"Clean Architecture Testing Approach"},{"location":"WORKFLOWS/","text":"GitHub Actions Workflows This document explains the GitHub Actions workflows used in the calibration-service project. Overview The project uses GitHub Actions for continuous integration and testing. The workflows are designed to validate code changes, run tests, and ensure the application behaves as expected. All workflows use uv for Python dependency management and virtual environment creation, providing fast and reliable dependency resolution. Workflows 1. CI Workflow ( ci.yaml ) This is the main CI workflow that runs on pushes to main, pull requests, and manual triggers. Jobs: python-tests : Runs Python unit and integration tests with coverage build : Builds the Docker image and uploads it as an artifact container-tests : Tests the Docker image functionality Features: Complete end-to-end testing (in future) Docker image validation Test report generation Code coverage analysis 2. Python Tests Workflow ( pytest.yaml ) This workflow focuses specifically on Python testing and runs when Python files or related configurations change. Jobs: test : Runs Python unit and integration tests with coverage Features: Faster feedback for Python code changes Test report generation Code coverage analysis Path-specific triggering to avoid unnecessary runs 3. Pyright Workflow ( pyright.yaml ) This workflow performs static type checking with Pyright to ensure type safety and catch issues early in development. Configuration is managed in pyproject.toml . Jobs: typecheck : Sets up Python environment with uv, then runs Pyright to check types across the codebase Features: Fast dependency resolution using uv Enforces type-safety/correctness and detects mismatches Catches potential runtime issues during development 4. Ruff Workflow ( ruff.yaml ) This workflow uses the ruff linter and ruff formatter to automatically enforce code quality standards. Ruff is configured via pyproject.toml and runs as a CI job to maintain a clean, consistent codebase. Jobs: ruff : Runs static analysis to enforce formatting, code style, and detect common issues Features: Enforces consistent formatting and PEP 8-style conventions Detects unused code, logic errors, and security concerns Helps catch issues early, before code is merged Test Reports Both the CI Workflow and the Python Tests Workflow generate and upload test reports as artifacts, which can be downloaded from the workflow run page. Running Tests Locally Before submitting a pull request, it's recommended to run the tests locally, or rely on the pre-commit git hook. For detailed information on how to run tests on localhost, see ./TESTS.md . CI/CD Pipeline Flow Code is pushed to GitHub GitHub Actions workflows are triggered: Ruff for linting and formatting Pyright for type checking Python Tests for unit and integration testing CI for complete validation including Docker builds Tests are run and reports are generated If all tests pass, the workflow succeeds Test reports and coverage information are available for review Future Enhancements Planned enhancements to the CI/CD pipeline include: Integration with deployment workflows Performance testing Security scanning End-to-end testing with ephemeral containers and/or external services","title":"CICD"},{"location":"WORKFLOWS/#github-actions-workflows","text":"This document explains the GitHub Actions workflows used in the calibration-service project.","title":"GitHub Actions Workflows"},{"location":"WORKFLOWS/#overview","text":"The project uses GitHub Actions for continuous integration and testing. The workflows are designed to validate code changes, run tests, and ensure the application behaves as expected. All workflows use uv for Python dependency management and virtual environment creation, providing fast and reliable dependency resolution.","title":"Overview"},{"location":"WORKFLOWS/#workflows","text":"","title":"Workflows"},{"location":"WORKFLOWS/#1-ci-workflow-ciyaml","text":"This is the main CI workflow that runs on pushes to main, pull requests, and manual triggers. Jobs: python-tests : Runs Python unit and integration tests with coverage build : Builds the Docker image and uploads it as an artifact container-tests : Tests the Docker image functionality Features: Complete end-to-end testing (in future) Docker image validation Test report generation Code coverage analysis","title":"1. CI Workflow (ci.yaml)"},{"location":"WORKFLOWS/#2-python-tests-workflow-pytestyaml","text":"This workflow focuses specifically on Python testing and runs when Python files or related configurations change. Jobs: test : Runs Python unit and integration tests with coverage Features: Faster feedback for Python code changes Test report generation Code coverage analysis Path-specific triggering to avoid unnecessary runs","title":"2. Python Tests Workflow (pytest.yaml)"},{"location":"WORKFLOWS/#3-pyright-workflow-pyrightyaml","text":"This workflow performs static type checking with Pyright to ensure type safety and catch issues early in development. Configuration is managed in pyproject.toml . Jobs: typecheck : Sets up Python environment with uv, then runs Pyright to check types across the codebase Features: Fast dependency resolution using uv Enforces type-safety/correctness and detects mismatches Catches potential runtime issues during development","title":"3. Pyright Workflow (pyright.yaml)"},{"location":"WORKFLOWS/#4-ruff-workflow-ruffyaml","text":"This workflow uses the ruff linter and ruff formatter to automatically enforce code quality standards. Ruff is configured via pyproject.toml and runs as a CI job to maintain a clean, consistent codebase. Jobs: ruff : Runs static analysis to enforce formatting, code style, and detect common issues Features: Enforces consistent formatting and PEP 8-style conventions Detects unused code, logic errors, and security concerns Helps catch issues early, before code is merged","title":"4. Ruff Workflow (ruff.yaml)"},{"location":"WORKFLOWS/#test-reports","text":"Both the CI Workflow and the Python Tests Workflow generate and upload test reports as artifacts, which can be downloaded from the workflow run page.","title":"Test Reports"},{"location":"WORKFLOWS/#running-tests-locally","text":"Before submitting a pull request, it's recommended to run the tests locally, or rely on the pre-commit git hook. For detailed information on how to run tests on localhost, see ./TESTS.md .","title":"Running Tests Locally"},{"location":"WORKFLOWS/#cicd-pipeline-flow","text":"Code is pushed to GitHub GitHub Actions workflows are triggered: Ruff for linting and formatting Pyright for type checking Python Tests for unit and integration testing CI for complete validation including Docker builds Tests are run and reports are generated If all tests pass, the workflow succeeds Test reports and coverage information are available for review","title":"CI/CD Pipeline Flow"},{"location":"WORKFLOWS/#future-enhancements","text":"Planned enhancements to the CI/CD pipeline include: Integration with deployment workflows Performance testing Security scanning End-to-end testing with ephemeral containers and/or external services","title":"Future Enhancements"}]}